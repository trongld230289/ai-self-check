const vscode = require('vscode');
const fs = require('fs');
const path = require('path');

// Token counting utilities
/**
 * Estimate token count from text using approximate ratios
 * @param {string} text - Input text to count tokens for
 * @returns {number} - Estimated token count
 */
function estimateTokenCount(text) {
    if (!text || typeof text !== 'string') return 0;
    
    // Approximate token-to-character ratios for different languages
    // English: ~4 characters per token
    // Code: ~3.5 characters per token (more dense)
    // Mixed content: ~3.8 characters per token
    
    const charCount = text.length;
    const avgCharsPerToken = 3.8;
    
    return Math.ceil(charCount / avgCharsPerToken);
}

/**
 * Normalize model family name for consistent pricing lookup
 * @param {string} modelFamily - Raw model family name
 * @returns {string} - Normalized model key
 */
function normalizeModelFamily(modelFamily) {
    if (!modelFamily || typeof modelFamily !== 'string') {
        return 'unknown';
    }
    
    const family = modelFamily.toLowerCase().trim();
    
    // Claude models
    if (family.includes('claude')) {
        if (family.includes('4')) return 'claude-sonnet-4';
        if (family.includes('3.7') && family.includes('thinking')) return 'claude-sonnet-3.7-thinking';
        if (family.includes('3.7')) return 'claude-sonnet-3.7';
        if (family.includes('3.5')) return 'claude-sonnet-3.5';
        if (family.includes('sonnet')) return 'claude-sonnet-3.5'; // Default claude
        return 'claude-sonnet-3.5';
    }
    
    // GPT models
    if (family.includes('gpt')) {
        if (family.includes('4o') && family.includes('mini')) return 'gpt-4o-mini';
        if (family.includes('4o')) return 'gpt-4o';
        if (family.includes('4') && family.includes('turbo')) return 'gpt-4-turbo';
        if (family.includes('4')) return 'gpt-4';
        return 'gpt-4o'; // Default GPT
    }
    
    // Gemini models
    if (family.includes('gemini')) {
        if (family.includes('2.0') && family.includes('flash')) return 'gemini-2.0-flash';
        if (family.includes('2-0') && family.includes('flash')) return 'gemini-2.0-flash';
        if (family.includes('2.5') && family.includes('pro')) return 'gemini-2.5-pro';
        if (family.includes('2-5') && family.includes('pro')) return 'gemini-2.5-pro';
        if (family.includes('flash')) return 'gemini-2.0-flash';
        return 'gemini-2.5-pro'; // Default gemini
    }
    
    // O-series models
    if (family.includes('o3') && family.includes('mini')) return 'o3-mini';
    if (family.includes('o4') && family.includes('mini')) return 'o4-mini';
    
    // Grok models
    if (family.includes('grok')) {
        if (family.includes('code') && family.includes('fast')) return 'grok-code-fast-1';
        return 'grok-code-fast-1';
    }
    
    // Unknown model
    return 'unknown';
}

/**
 * Get estimated cost for token usage based on model
 * @param {number} inputTokens - Input token count
 * @param {number} outputTokens - Output token count
 * @param {string} modelFamily - Model family name
 * @returns {object} - Cost breakdown
 */
function estimateCost(inputTokens, outputTokens, modelFamily) {
    // Pricing per 1M tokens (last updated: 2025-10-07 - GitHub Copilot rates)
    const pricing = {
        // Claude Models (Anthropic via Copilot)
        'claude-3-5-sonnet': { input: 3.0, output: 15.0 },
        'claude-sonnet-3.5': { input: 3.0, output: 15.0 },
        'claude-3-7-sonnet': { input: 3.0, output: 15.0 },
        'claude-sonnet-3.7': { input: 3.0, output: 15.0 },
        'claude-3-7-thinking': { input: 3.75, output: 18.75 }, // Extended thinking
        'claude-sonnet-3.7-thinking': { input: 3.75, output: 18.75 },
        'claude-4': { input: 3.0, output: 15.0 },
        'claude-sonnet-4': { input: 3.0, output: 15.0 },
        'claude-4.5': { input: 3.0, output: 15.0 },
        'claude-sonnet-4.5': { input: 3.0, output: 15.0 },
        'claude-3-opus': { input: 15.0, output: 75.0 },
        'claude-opus': { input: 15.0, output: 75.0 },
        'claude-3-haiku': { input: 0.25, output: 1.25 },
        'claude-haiku': { input: 0.25, output: 1.25 },
        
        // GPT Models (OpenAI via Copilot) 
        'gpt-4.1': { input: 2.5, output: 10.0 },
        'gpt-4o': { input: 2.5, output: 10.0 },
        'gpt-4o-mini': { input: 0.15, output: 0.6 },
        'gpt-4-turbo': { input: 10.0, output: 30.0 },
        'gpt-4': { input: 2.5, output: 10.0 }, // Fixed: was showing 30/60, should be 2.5/10
        'o1': { input: 15.0, output: 60.0 },
        'o1-mini': { input: 3.0, output: 12.0 },
        'o1-preview': { input: 15.0, output: 60.0 },
        
        // Gemini Models (Google via Copilot)
        'gemini-2.0-flash': { input: 0.075, output: 0.3 },
        'gemini-2-0-flash': { input: 0.075, output: 0.3 },
        'gemini-2.0-flash-exp': { input: 0.075, output: 0.3 },
        'gemini-2.5-pro': { input: 1.25, output: 5.0 },
        'gemini-2-5-pro': { input: 1.25, output: 5.0 },
        'gemini-2.5-flash': { input: 0.075, output: 0.3 },
        'gemini-2-5-flash': { input: 0.075, output: 0.3 },
        'gemini': { input: 1.25, output: 5.0 }, // Default gemini
        
        // O-series Models (OpenAI reasoning)
        'o3-mini': { input: 1.1, output: 4.4 },
        'o4-mini': { input: 1.1, output: 4.4 },
        'o4-mini-preview': { input: 1.1, output: 4.4 },
        
        // DeepSeek Models
        'deepseek-r1': { input: 0.55, output: 2.19 },
        'deepseek-r1-distill-qwen': { input: 0.14, output: 0.55 },
        'deepseek-r1-distill-llama': { input: 0.14, output: 0.55 },
        
        // Grok Models (xAI)
        'grok-code-fast-1': { input: 1.0, output: 3.0 },
        'grok-code-fast-1-preview': { input: 1.0, output: 3.0 },
        
        // Unknown/Fallback
        'unknown': { input: 0, output: 0 }
    };
    
    // Normalize model family name for lookup
    const normalizedFamily = normalizeModelFamily(modelFamily);
    console.log(`üîç Cost calculation - Original: "${modelFamily}" -> Normalized: "${normalizedFamily}"`);
    
    // Get pricing or fallback to unknown
    const modelPricing = pricing[normalizedFamily] || pricing['unknown'];
    console.log(`üí∞ Pricing found:`, modelPricing);
    
    const inputCost = (inputTokens / 1000000) * modelPricing.input;
    const outputCost = (outputTokens / 1000000) * modelPricing.output;
    const totalCost = inputCost + outputCost;
    
    return {
        inputCost: inputCost,
        outputCost: outputCost,
        totalCost: totalCost,
        inputTokens: inputTokens,
        outputTokens: outputTokens,
        pricing: modelPricing // Include pricing info for display
    };
}

/**
 * Strip internal blocks from AI output before displaying to user
 * @param {string} output - The raw AI output text
 * @returns {string} - Cleaned output with internal blocks removed
 */
function stripInternalBlocks(output) {
    // Remove <INTERNAL_ONLY>...</INTERNAL_ONLY> blocks
    let cleanedOutput = output.replace(/<INTERNAL_ONLY>[\s\S]*?<\/INTERNAL_ONLY>/g, '');
    
    // // Remove HTML comments (<!-- ... -->)
    // cleanedOutput = cleanedOutput.replace(/<!--[\s\S]*?-->/g, '');

    // // Clean up multiple newlines
    // cleanedOutput = cleanedOutput.replace(/\n{3,}/g, '\n\n');
    
    return cleanedOutput.trim();
}

// Unified fallback model selection function
async function getFallbackModel(currentModel, stream, attemptNumber = 1) {
    try {
        // Get all available models
        const allModels = await vscode.lm.selectChatModels();
        const availableModels = allModels.filter(m => m.id !== currentModel.id);
        
        if (availableModels.length === 0) {
            throw new Error('No alternative models available for fallback');
        }
        
        // Fallback priority order (respecting user choice):
        // 1. Available Claude models
        // 2. Available GPT models
        // 3. First available model
        
        let fallbackModel = null;
        
        // Priority 1: Claude models (if available)
        fallbackModel = availableModels.find(m => 
            m.family.toLowerCase().includes('claude') && 
            (m.family.includes('4') || m.family.toLowerCase().includes('sonnet'))
        );
        
        // Priority 2: GPT models (if no Claude available)
        if (!fallbackModel) {
            fallbackModel = availableModels.find(m => 
                m.family.toLowerCase().includes('gpt') && 
                (m.family.includes('4o') || m.family.includes('4'))
            );
        }
        
        // Priority 3: First available model
        if (!fallbackModel) {
            fallbackModel = availableModels[0];
        }
        
        // Show detailed fallback information with reasoning
        const availableList = availableModels.map(m => m.family).join(', ');
        
        console.log(`üîÑ Fallback ${attemptNumber}: ${currentModel.family} ‚Üí ${fallbackModel.family}`);
        
        // Show user-friendly fallback messaging
        stream.markdown(`üîÑ **Switching models** (attempt ${attemptNumber}):\n`);
        stream.markdown(`   ‚ùå **Previous:** ${currentModel.family} (failed)\n`);
        stream.markdown(`   ‚úÖ **New:** ${fallbackModel.family} (selected as fallback)\n\n`);
        
        // Show fallback reasoning
        if (fallbackModel.family.toLowerCase().includes('claude')) {
            stream.markdown(`üí° **Fallback reason:** Switched to Claude model (Priority 1 fallback)\n\n`);
        } else if (fallbackModel.family.toLowerCase().includes('gpt')) {
            stream.markdown(`üí° **Fallback reason:** Switched to GPT model (Priority 2 fallback)\n\n`);
        } else {
            stream.markdown(`üí° **Fallback reason:** Using first available model (Priority 3 fallback)\n\n`);
        }
        
        stream.markdown(`üìã **All available models:** ${availableList}\n\n`);
        stream.markdown('‚è≥ **Continuing analysis with new model...**\n\n');
        
        return fallbackModel;
        
    } catch (error) {
        console.error('‚ùå Fallback model selection failed:', error);
        stream.markdown(`‚ùå **Fallback failed:** ${error.message}\n\n`);
        throw error;
    }
}

/**
 * Unified AI execution function with token counting, streaming, and fallback logic
 * @param {string} userPrompt - The prompt to send to the AI model
 * @param {object} model - The AI model to use
 * @param {object} stream - Chat stream for output
 * @param {string} reviewType - Type of review ('Changes' or 'File')
 * @param {number} attemptCount - Current attempt number for recursion
 * @returns {Promise<boolean>} - true if successful, false if failed
 */
async function executeAIReview(userPrompt, model, stream, reviewType = 'Analysis', attemptCount = 0, originalRequestedModel = null) {
    const maxAttempts = 3;
    
    try {
        // Calculate input tokens and estimated cost (for final summary only)
        const inputTokens = estimateTokenCount(userPrompt);
        const messages = [vscode.LanguageModelChatMessage.User(userPrompt)];
        
        console.log(`üì§ Attempt ${attemptCount + 1}: Sending request to model:`, model.family);
        
        const chatResponse = await model.sendRequest(messages, {}, new vscode.CancellationTokenSource().token);
        
        // Show successful model usage with accurate status  
        const reviewIcon = reviewType === 'Changes' ? 'üîç' : 'üìÑ';
        
        // Determine if this is really the originally requested model
        let modelStatus;
        if (attemptCount === 0) {
            // First attempt - check if this matches original request
            if (originalRequestedModel && model.family === originalRequestedModel) {
                modelStatus = 'selected model';
            } else if (originalRequestedModel && model.family !== originalRequestedModel) {
                modelStatus = `fallback model (${originalRequestedModel} not available)`;
            } else {
                // No original model info, assume this is user's choice
                modelStatus = 'selected model';
            }
        } else {
            // Subsequent attempts are always fallbacks
            modelStatus = `fallback model (attempt ${attemptCount + 1})`;
        }
        
        stream.markdown(`ü§ñ **Streaming ${reviewType} ${reviewIcon}** (using ${model.family} - ${modelStatus}):\n\n`);
        
        let fragmentCount = 0;
        let outputText = '';
        
        // Stream output in real-time
        stream.markdown('---\n\n');
        
        for await (const fragment of chatResponse.text) {
            outputText += fragment;
            fragmentCount++;
            
            // Stream each fragment immediately for real-time display
            stream.markdown(fragment);
            
            // Progress indicator every 50 fragments
            if (fragmentCount % 50 === 0) {
                console.log(`Streaming progress: ${fragmentCount} fragments processed`);
            }
        }
        
        // TODO: Clean the output to remove internal blocks (temporarily disabled for streaming)
        // const cleanedOutput = stripInternalBlocks(outputText);
        // console.log(`üßπ Cleaned output: ${outputText.length} chars -> ${cleanedOutput.length} chars`);
        
        // Show completion with token statistics
        showTokenStatistics(stream, userPrompt, outputText, model, reviewType, attemptCount);
        
        return true; // Success
        
    } catch (modelError) {
        console.error(`‚ùå Model ${model.family} failed (attempt ${attemptCount + 1}):`, modelError);
        
        // If we've reached max attempts, throw the error
        if (attemptCount >= maxAttempts - 1) {
            throw new Error(`All ${maxAttempts} model attempts failed. Last error: ${modelError.message}`);
        }
        
        // Handle quota/rate limit errors
        if (modelError.message && (
            modelError.message.includes('exhausted your premium model quota') ||
            modelError.message.includes('quota') ||
            modelError.message.includes('rate limit') ||
            modelError.message.includes('too many requests') ||
            modelError.message.includes('limit exceeded')
        )) {
            stream.markdown(`üö´ **${model.family} quota exceeded** - Switching to alternative model...\n\n`);
            stream.markdown(`üí° **Reason:** Premium model quota exhausted. Automatically falling back to available model.\n\n`);
            
            try {
                const fallbackModel = await getFallbackModel(model, stream, attemptCount + 1);
                return await executeAIReview(userPrompt, fallbackModel, stream, reviewType, attemptCount + 1, originalRequestedModel || model.family);
            } catch (fallbackError) {
                stream.markdown(`‚ùå **Fallback failed:** ${fallbackError.message}\n\n`);
                throw new Error(`Quota exceeded and fallback failed: ${fallbackError.message}`);
            }
        }
        // Handle model not supported errors
        else if (modelError.message && (
            modelError.message.includes('model_not_supported') || 
            modelError.message.includes('not supported') ||
            modelError.message.includes('model is not available') ||
            modelError.message.includes('The requested model is not supported')
        )) {
            stream.markdown(`‚ùå **${model.family} not supported** - Switching to alternative model...\n\n`);
            stream.markdown(`üí° **Reason:** Model not available in current environment. Automatically falling back.\n\n`);
            
            try {
                const fallbackModel = await getFallbackModel(model, stream, attemptCount + 1);
                return await executeAIReview(userPrompt, fallbackModel, stream, reviewType, attemptCount + 1, originalRequestedModel || model.family);
            } catch (fallbackError) {
                stream.markdown(`‚ùå **Fallback failed:** ${fallbackError.message}\n\n`);
                throw new Error(`Model not supported and fallback failed: ${fallbackError.message}`);
            }
        } else {
            // Other errors, don't retry
            stream.markdown(`‚ùå **${model.family} failed** (attempt ${attemptCount + 1}): ${modelError.message}\n\n`);
            throw modelError;
        }
    }
}

/**
 * Get review template (simplified - templates already combined in workspace)
 * @param {string} templateName - Name of template file (e.g., 'review-changes.md')
 * @param {object} context - VS Code extension context
 * @returns {object} - {content: string, source: string, path: string}
 */
function getReviewTemplate(templateName, context = null) {
    const workspaceFolder = vscode.workspace.workspaceFolders?.[0];
    
    // Determine template paths - workspace first, then extension fallback
    let templatePath, templateSource;
    
    if (workspaceFolder) {
        templatePath = path.join(workspaceFolder.uri.fsPath, 'instructions', templateName);
        templateSource = 'workspace';
        
        // If workspace template doesn't exist, try extension templates
        if (!fs.existsSync(templatePath) && context) {
            templatePath = path.join(context.extensionPath, 'templates', templateName);
            templateSource = 'extension';
        }
    } else if (context) {
        // No workspace, use extension templates directly
        templatePath = path.join(context.extensionPath, 'templates', templateName);
        templateSource = 'extension';
    } else {
        throw new Error('No workspace folder or extension context available');
    }
    
    try {
        if (fs.existsSync(templatePath)) {
            const content = fs.readFileSync(templatePath, 'utf8');
            console.log(`‚úÖ Loaded template from: ${templatePath}`);
            
            return {
                content: content,
                source: templateSource,
                path: templatePath
            };
        } else {
            throw new Error(`Template not found at: ${templatePath}`);
        }
        
    } catch (error) {
        console.error('‚ùå Template loading error:', error);
        throw new Error(`Failed to load template: ${error.message}`);
    }
}

/**
 * Unified model detection function used by both @review-file and @review-changes
 * @param {object} stream - Chat stream for output
 * @param {object} requestedModel - Requested model object
 * @param {object} chatContext - VS Code chat context
 * @param {object} request - VS Code request object
 * @returns {Promise<object|null>} - Selected AI model or null
 */
async function getUnifiedModel(stream, requestedModel = null, chatContext = null, request = null) {
    let models = [];
    
    console.log('=== getUnifiedModel DEBUG START ===');
    console.log('üîç Full request object:', JSON.stringify(request, null, 2));
    console.log('üîç Request keys:', request ? Object.keys(request) : 'null');
    console.log('üîç Request.model:', request?.model);
    console.log('üîç Request.model?.family:', request?.model?.family);
    console.log('üîç Request.model?.id:', request?.model?.id);
    
    // Check ALL possible model locations in request
    if (request) {
        console.log('üîç Checking all request properties for model...');
        for (const key in request) {
            console.log(`   - request.${key}:`, typeof request[key] === 'object' ? JSON.stringify(request[key]).substring(0, 200) : request[key]);
        }
    }
    
    // PRIORITY 1: Check request.model first (this is where VS Code puts the selected model!)
    if (request && request.model) {
        console.log('‚úÖ Found model in request.model:', request.model);
        console.log('‚úÖ Model family:', request.model.family);
        console.log('‚úÖ Model id:', request.model.id);
        console.log('üéØ Using user-selected model from VS Code chat');
        
        // Add warning if user selected GPT but wanted Claude
        if (request.model.family && request.model.family.toLowerCase().includes('gpt')) {
            console.log('‚ÑπÔ∏è NOTE: User selected GPT model. To use Claude, please select a Claude model in VS Code chat dropdown.');
        }
        
        return request.model;
    } else {
        console.log('‚ùå No model found in request.model');
        console.log('‚ùå Request is:', request ? 'valid object' : 'null/undefined');
        console.log('‚ùå Request.model is:', request?.model ? 'valid' : 'null/undefined');
    }
    
    // PRIORITY 2: Use passed requestedModel object
    if (requestedModel && typeof requestedModel === 'object' && requestedModel.family) {
        console.log('Using requested model object:', requestedModel);
        return requestedModel;
    }
    
    // PRIORITY 3: Try to get the current model from chat context  
    if (chatContext) {
        console.log('üîç Checking for model in chatContext...');
        
        // VS Code chat context may have model at chatContext.model or chatContext.participant.model
        let currentModel = null;
        
        if (chatContext.model) {
            console.log('‚úÖ Found model at chatContext.model:', chatContext.model);
            currentModel = chatContext.model;
        } else {
            console.log('‚ùå No model found at chatContext.model');
        }
        
        if (chatContext.participant && chatContext.participant.model) {
            console.log('‚úÖ Found model at chatContext.participant.model:', chatContext.participant.model);
            currentModel = chatContext.participant.model;
        } else {
            console.log('‚ùå No model found at chatContext.participant.model');
        }
        
        // Check other possible locations
        if (chatContext.request && chatContext.request.model) {
            console.log('‚úÖ Found model at chatContext.request.model:', chatContext.request.model);
            currentModel = chatContext.request.model;
        }
        
        if (currentModel) {
            console.log('Using selected chatbox model:', currentModel);
            return currentModel;
        } else {
            console.log('‚ö†Ô∏è No model found in any chatContext location');
        }
    }
    
    // PRIORITY 4: Fallback to available models detection
    // NOTE: Only use this if NO model found in context (user hasn't selected anything)
    console.log('üîÑ Falling back to available models detection...');
    
    // Get available models and try to find the currently selected one
    try {
        models = await vscode.lm.selectChatModels();
        console.log('üìã All available models:', models.map(m => `${m.family || m.id} (${m.vendor || 'Unknown'})`));
        
        if (models.length > 0) {
            // FALLBACK 1: Smart model selection based on quality preference (respecting user choice)
            const modelPreferences = [
                // Tier 1: High-quality models (Claude and GPT)
                'claude-sonnet-4', 'claude-4', 'claude-3.5-sonnet', 'claude-3.7-sonnet',
                'gpt-4o', 'gpt-4-turbo', 'gpt-4',
                // Tier 2: Other advanced models
                'gpt-4o-mini', 'o4-mini', 'o3-mini', 'gemini-2.5-pro', 'gemini-2.0-flash', 
                // Tier 3: Fallback options
                'gpt-3.5-turbo', 'grok-code'
            ];

            let selectedModel = null;
            
            // Try to find best available model based on preference order
            for (const preference of modelPreferences) {
                selectedModel = models.find(m => {
                    const modelName = (m.family || m.id).toLowerCase();
                    const prefLower = preference.toLowerCase();
                    
                    // Exact match or contains match
                    return modelName === prefLower || 
                           modelName.includes(prefLower) ||
                           modelName.replace(/[-_]/g, '').includes(prefLower.replace(/[-_]/g, ''));
                });
                
                if (selectedModel) {
                    console.log(`‚úÖ Found preferred model: ${selectedModel.family || selectedModel.id} (preference: ${preference})`);
                    break;
                }
            }

            // If no preferred model found, use first available
            if (!selectedModel && models.length > 0) {
                selectedModel = models[0];
                console.log(`‚ö†Ô∏è No preferred model found, using first available: ${selectedModel.family || selectedModel.id}`);
            }

            if (selectedModel) {
                models = [selectedModel];
            }
        }
    } catch (error) {
        console.log('Error getting models:', error);
        models = [];
    }
    
    const finalModel = models.length > 0 ? models[0] : null;
    console.log('üèÅ Final model selected:', finalModel);
    console.log('=== getUnifiedModel DEBUG END ===');
    
    return finalModel;
}

/**
 * Reusable function for displaying review headers
 * @param {object} stream - Chat stream for output
 * @param {string} reviewType - Type of review (e.g., 'Changes Review', 'File Review')
 * @param {string} filePath - Path to the file being reviewed
 * @param {number|null} diffLength - Length of diff content (for changes review)
 * @param {string|null} templatePath - Path to the template being used
 */
function displayReviewHeader(stream, reviewType, filePath, diffLength = null, templatePath = null) {
    const path = require('path');
    const fileName = path.basename(filePath);
    const fullPath = filePath;
    
    stream.markdown(`# üîç ${reviewType}: \`${fileName}\`\n\n`);
    stream.markdown(`üìÇ **Full Path:** \`${fullPath}\`\n\n`);
    
    if (diffLength !== null) {
        stream.markdown('## üìã Current Changes Review\n\n');
        stream.markdown(`**Diff Length:** ${diffLength} characters\n\n`);
        if (templatePath) {
            const templateFile = templatePath.replace(/.*[\\\/]/, '');
            const templateDir = templatePath.includes('instructions') ? 'instructions' : 'extension/templates';
            stream.markdown(`**Template used:** ${templateDir}/${templateFile}\n\n`);
        }
        stream.markdown('---\n\n');
    } else {
        stream.markdown('## üìÑ File Review Analysis\n\n');
        if (templatePath) {
            const templateFile = templatePath.replace(/.*[\\\/]/, '');
            const templateDir = templatePath.includes('instructions') ? 'instructions' : 'extension/templates';
            stream.markdown(`**Template used:** ${templateDir}/${templateFile}\n\n`);
        }
    }
}

/**
 * Get checklist status icon based on status
 * @param {string} status - Status value (ok, warning, critical, suggestion, unknown)
 * @returns {string} - Corresponding emoji icon
 */
function getChecklistStatusIcon(status) {
    const statusIcons = {
        'ok': '‚úÖ',
        'warning': '‚ö†Ô∏è',
        'critical': 'üö®', 
        'suggestion': 'üí°',
        'unknown': '‚ùì'
    };
    return statusIcons[status] || '‚ùì';
}

/**
 * Get all available language models from VS Code API
 * @returns {Promise<Array>} Array of available models with details
 */
async function getAvailableModels() {
    try {
        const models = await vscode.lm.selectChatModels();
        return models.map(model => ({
            id: model.id,
            family: model.family || model.id,
            name: model.name || model.family || model.id,
            vendor: model.vendor,
            version: model.version,
            maxInputTokens: model.maxInputTokens,
            countTokens: model.countTokens ? 'supported' : 'not supported'
        }));
    } catch (error) {
        console.error('Error fetching available models:', error);
        return [];
    }
}

/**
 * Display available models information
 */
async function showAvailableModels() {
    try {
        const models = await getAvailableModels();
        
        if (models.length === 0) {
            vscode.window.showInformationMessage('‚ùå No language models available');
            return;
        }

        const modelList = models.map(model => 
            `‚Ä¢ **${model.family}** (${model.id})\n  - Vendor: ${model.vendor || 'Unknown'}\n  - Max tokens: ${model.maxInputTokens || 'Unknown'}\n  - Token counting: ${model.countTokens}`
        ).join('\n\n');

        const message = `ü§ñ **Available Language Models** (${models.length} found):\n\n${modelList}`;
        
        // Show in information message (for quick view)
        const action = await vscode.window.showInformationMessage(
            `Found ${models.length} available language models`,
            'Show Details',
            'Copy List'
        );

        if (action === 'Show Details') {
            // Create and show in untitled document for detailed view
            const doc = await vscode.workspace.openTextDocument({
                content: message,
                language: 'markdown'
            });
            await vscode.window.showTextDocument(doc);
        } else if (action === 'Copy List') {
            // Copy to clipboard
            const simpleList = models.map(m => `${m.family} (${m.id})`).join('\n');
            await vscode.env.clipboard.writeText(simpleList);
            vscode.window.showInformationMessage('‚úÖ Model list copied to clipboard');
        }

    } catch (error) {
        console.error('Error showing available models:', error);
        vscode.window.showErrorMessage(`Error fetching models: ${error.message}`);
    }
}

/**
 * Show token usage statistics and completion status
 * @param {object} stream - Chat stream for output
 * @param {string} inputText - Input text sent to AI
 * @param {string} outputText - Output text received from AI
 * @param {object} model - AI model used
 * @param {string} reviewType - Type of analysis
 * @param {number} attemptCount - Current attempt number
 */
function showTokenStatistics(stream, inputText, outputText, model, reviewType, attemptCount) {
    // Calculate input and output tokens
    const inputTokens = estimateTokenCount(inputText);
    const outputTokens = estimateTokenCount(outputText);
    const finalCost = estimateCost(inputTokens, outputTokens, model.family);
    
    // Determine completion status
    const completionStatus = attemptCount === 0 ? 'original model' : `fallback model (${model.family})`;
    
    // Display token statistics
    stream.markdown(`\n\n---\n\n`);
    stream.markdown(`üìä **Token Usage Summary:**\n`);
    stream.markdown(`- **Input tokens**: ${inputTokens.toLocaleString()} ($${finalCost.pricing.input.toFixed(2)}/1M)\n`);
    stream.markdown(`- **Output tokens**: ${outputTokens.toLocaleString()} ($${finalCost.pricing.output.toFixed(2)}/1M tokens)\n`);
    stream.markdown(`- **Total tokens**: ${(inputTokens + outputTokens).toLocaleString()}\n`);
    stream.markdown(`- **Estimated cost**: $${finalCost.totalCost.toFixed(4)} (Input: $${finalCost.inputCost.toFixed(4)} | Output: $${finalCost.outputCost.toFixed(4)})\n`);
    stream.markdown(`- **Model used**: ${model.family}\n\n`);
    stream.markdown(`‚úÖ **${reviewType} Analysis complete** - successfully used ${completionStatus}\n\n`);
    
    // Console logging
    console.log(`‚úÖ ${reviewType} review completed successfully with model:`, model.family);
    console.log(`üìä Token usage - Input: ${inputTokens}, Output: ${outputTokens}, Cost: $${finalCost.totalCost.toFixed(4)}`);
}

/**
 * Execute API call for scan-app pattern analysis
 * @param {string} prompt - The prompt to send to the AI model
 * @param {object} stream - Chat stream for output
 * @param {string} analysisType - Type of analysis (e.g. 'API Pattern Analysis')
 * @returns {Promise<object|null>} - Parsed AI response or null if failed
 */
async function executeApiAnalysisCall(prompt, stream, analysisType = 'API Pattern Analysis') {
    try {
        // Get AI model
        const model = await getUnifiedModel(stream);
        if (!model) {
            stream.markdown('‚ùå **No AI model available** - Please select a model in VS Code\n\n');
            return null;
        }
        
        console.log(`üì§ Starting ${analysisType} with model:`, model.family);
        
        // Show analysis start
        stream.markdown(`ü§ñ **${analysisType}** (using ${model.family}):\n\n`);
        stream.markdown('üîÑ **Processing with AI...**\n\n');
        
        // Execute AI call
        const messages = [vscode.LanguageModelChatMessage.User(prompt)];
        const chatResponse = await model.sendRequest(messages, {}, new vscode.CancellationTokenSource().token);
        
        // Collect output
        let outputText = '';
        for await (const fragment of chatResponse.text) {
            outputText += fragment;
        }
        
        // Show token statistics
        showTokenStatistics(stream, prompt, outputText, model, analysisType, 0);
        
        // Try to parse JSON response
        try {
            const parsedResponse = JSON.parse(outputText);
            console.log(`‚úÖ ${analysisType} JSON parsed successfully`);
            return parsedResponse;
        } catch (parseError) {
            console.log(`‚ö†Ô∏è ${analysisType} response is not JSON, returning raw text`);
            return { rawResponse: outputText };
        }
        
    } catch (error) {
        console.error(`‚ùå ${analysisType} failed:`, error);
        stream.markdown(`‚ùå **${analysisType} failed:** ${error.message}\n\n`);
        
        // Try fallback if quota/model error
        if (error.message && (
            error.message.includes('quota') ||
            error.message.includes('rate limit') ||
            error.message.includes('not supported')
        )) {
            stream.markdown(`üîÑ **Attempting fallback model...**\n\n`);
            try {
                const fallbackModel = await getFallbackModel(model, stream, 1);
                const messages = [vscode.LanguageModelChatMessage.User(prompt)];
                const chatResponse = await fallbackModel.sendRequest(messages, {}, new vscode.CancellationTokenSource().token);
                
                let outputText = '';
                for await (const fragment of chatResponse.text) {
                    outputText += fragment;
                }
                
                showTokenStatistics(stream, prompt, outputText, fallbackModel, analysisType, 1);
                
                try {
                    const parsedResponse = JSON.parse(outputText);
                    console.log(`‚úÖ ${analysisType} fallback JSON parsed successfully`);
                    return parsedResponse;
                } catch (parseError) {
                    return { rawResponse: outputText };
                }
                
            } catch (fallbackError) {
                stream.markdown(`‚ùå **Fallback also failed:** ${fallbackError.message}\n\n`);
                return null;
            }
        }
        
        return null;
    }
}

/**
 * Get programming language identifier from file extension
 * @param {string} ext - File extension (with or without dot)
 * @returns {string} - Language identifier for syntax highlighting
 */
function getLanguageFromExtension(ext) {
    const languageMap = {
        '.ts': 'typescript',
        '.js': 'javascript',
        '.html': 'html',
        '.css': 'css',
        '.scss': 'scss',
        '.json': 'json',
        '.md': 'markdown',
        '.py': 'python',
        '.java': 'java',
        '.cs': 'csharp'
    };
    return languageMap[ext] || 'text';
}

module.exports = {
    executeAIReview,
    getFallbackModel,
    estimateTokenCount,
    estimateCost,
    normalizeModelFamily,
    stripInternalBlocks,
    getReviewTemplate,
    displayReviewHeader,
    getUnifiedModel,
    getChecklistStatusIcon,
    getAvailableModels,
    showAvailableModels,
    getLanguageFromExtension,
    showTokenStatistics,
    executeApiAnalysisCall
};
